# Cloud-Native Geospatial Analytics

> The definitive reference for modern, cloud-native geospatial data engineering and analytics -- from GeoParquet and DuckDB Spatial to STAC, xarray, and planet-scale processing with Apache Sedona.

**Scope.** This guide covers the tools, formats, APIs, and architectural patterns that define the 2024-2026 cloud-native geospatial stack. Every section contains runnable code, benchmark data, and cross-references to sibling pages in this repository.

**Prerequisites.** Familiarity with Python, basic SQL, and core GIS concepts (CRS, vector vs raster, spatial indexing). For foundational libraries see [Python Geospatial Stack](python-stack.md); for data sources see [../data-sources/](../data-sources/README.md).

---

## Table of Contents

| # | Section | Key Technologies |
|---|---------|-----------------|
| 1 | [Introduction and Quick Picks](#1-introduction-and-quick-picks) | Overview, decision matrix |
| 2 | [GeoParquet and GeoArrow](#2-geoparquet-and-geoarrow) | Apache Parquet, Arrow, GeoPandas |
| 3 | [DuckDB Spatial](#3-duckdb-spatial) | DuckDB, spatial SQL, H3, WASM |
| 4 | [Cloud-Optimized GeoTIFF (COG)](#4-cloud-optimized-geotiff-cog) | GDAL, rioxarray, tiling |
| 5 | [STAC Ecosystem](#5-stac-ecosystem) | pystac-client, stackstac, odc-stac |
| 6 | [xarray for Geospatial](#6-xarray-for-geospatial) | rioxarray, Zarr, Dask |
| 7 | [Apache Sedona](#7-apache-sedona) | Spark, distributed spatial SQL |
| 8 | [Dask-GeoPandas](#8-dask-geopandas) | Parallel vector, out-of-core |
| 9 | [FlatGeobuf](#9-flatgeobuf) | Streaming filter, HTTP range |
| 10 | [Google Earth Engine](#10-google-earth-engine) | ee API, geemap |
| 11 | [Microsoft Planetary Computer](#11-microsoft-planetary-computer) | STAC, Dask Gateway |
| 12 | [Overture Maps](#12-overture-maps) | DuckDB, building footprints |
| 13 | [Data Pipeline Patterns](#13-data-pipeline-patterns) | ETL, Dagster, Kart, CI/CD |
| 14 | [Performance Benchmarks](#14-performance-benchmarks) | Format comparison tables |
| 15 | [Cost Optimization](#15-cost-optimization) | Free tiers, serverless, caching |

---

## 1. Introduction and Quick Picks

### 1.1 What "Cloud-Native Geospatial" Means

Cloud-native geospatial is a philosophy: **bring the compute to the data, not the data to the compute.** Instead of downloading a 200 GB Shapefile, unzipping it, loading it into PostGIS, and running a query, you issue a SQL statement against a remote Parquet file and only the bytes you need travel over the network.

Core principles:

| Principle | Traditional | Cloud-Native |
|-----------|------------|--------------|
| Data locality | Download first, query later | Query in place (HTTP range requests) |
| Format design | Row-oriented, full-scan | Columnar, predicate pushdown, spatial index |
| Metadata | Sidecar files (.prj, .shx, .dbf) | Self-describing (embedded schema, CRS) |
| Streaming | Not possible | Partial reads via byte ranges |
| Interop | Format-specific drivers | Arrow IPC as lingua franca |
| Scale | Single machine | Embarrassingly parallel (Spark, Dask) |

### 1.2 The Modern Stack at a Glance

```
 +------------------------------------------------------+
 |                   Applications                        |
 |  Jupyter / QGIS / Kepler.gl / deck.gl / Felt         |
 +------------------------------------------------------+
 |                   Compute                             |
 |  DuckDB | GeoPandas | Sedona | Dask | xarray | EE   |
 +------------------------------------------------------+
 |                   Formats                             |
 |  GeoParquet | COG | Zarr | FlatGeobuf | PMTiles      |
 +------------------------------------------------------+
 |                   Catalogs                            |
 |  STAC | Overture | Planetary Computer | GEE Catalog   |
 +------------------------------------------------------+
 |                   Storage                             |
 |  S3 | GCS | Azure Blob | MinIO | Local FS            |
 +------------------------------------------------------+
```

### 1.3 Quick-Pick Decision Matrix

**"I want to..."**

| Goal | Recommended Tool | Format | Section |
|------|-----------------|--------|---------|
| Query 500M building footprints by bounding box | DuckDB Spatial | GeoParquet | [3](#3-duckdb-spatial) |
| Analyze a 10-year Landsat time series | xarray + Dask | COG via STAC | [5](#5-stac-ecosystem), [6](#6-xarray-for-geospatial) |
| Join census tracts to POI data (100 GB) | Apache Sedona | GeoParquet | [7](#7-apache-sedona) |
| Stream filtered features to a web map | FlatGeobuf over HTTP | FlatGeobuf | [9](#9-flatgeobuf) |
| Run NDVI on all Sentinel-2 tiles for a country | Google Earth Engine | N/A (server-side) | [10](#10-google-earth-engine) |
| Build an ETL pipeline for nightly parcel updates | Dagster + GDAL + DuckDB | GeoParquet | [13](#13-data-pipeline-patterns) |
| Version-control a GeoPackage collaboratively | Kart | GPKG (internally) | [13](#13-data-pipeline-patterns) |
| Serve tiles from a static file on S3 | PMTiles | PMTiles | [../js-bindbox/tile-servers.md](../js-bindbox/tile-servers.md) |

### 1.4 Installation Cheat Sheet

```bash
# Core cloud-native analytics environment
pip install geopandas pyarrow duckdb pystac-client stackstac \
    rioxarray xarray dask[complete] odc-stac planetary-computer \
    lonboard leafmap geemap

# Or with conda (recommended for GDAL linkage)
conda create -n cloudgeo python=3.12
conda activate cloudgeo
conda install -c conda-forge geopandas duckdb python-duckdb pyarrow \
    pystac-client stackstac rioxarray zarr dask planetary-computer

# DuckDB CLI (standalone)
# macOS
brew install duckdb
# Linux
curl -LO https://github.com/duckdb/duckdb/releases/latest/download/duckdb_cli-linux-amd64.zip
unzip duckdb_cli-linux-amd64.zip && sudo mv duckdb /usr/local/bin/

# Verify
python -c "import duckdb; duckdb.sql('INSTALL spatial; LOAD spatial; SELECT ST_Point(0,0);').show()"
```

---

## 2. GeoParquet and GeoArrow

> See also: [../js-bindbox/data-formats-loading.md](../js-bindbox/data-formats-loading.md) for browser-side Parquet reading.

### 2.1 Why GeoParquet Matters

GeoParquet is Apache Parquet with a standardized metadata convention for encoding geometry columns. It is the single most important format shift in the geospatial industry since the Shapefile.

| Feature | Shapefile | GeoJSON | GeoPackage | GeoParquet |
|---------|-----------|---------|------------|------------|
| Max file size | 2 GB | RAM-bound | No limit | No limit |
| Column types | Limited (10-char names) | String/Number | Full SQL types | Full Arrow types |
| CRS embedded | .prj sidecar | Usually missing | Yes | Yes (projjson) |
| Columnar | No | No | No | Yes |
| Predicate pushdown | No | No | SQLite index | Row group stats + bbox |
| Cloud-readable | No | Technically yes | No | Yes (HTTP range) |
| Spatial index | .shx (offset only) | No | R-tree | bbox covering (1.1) |
| Compression | None | gzip (external) | None | Snappy/Zstd/LZ4 |
| Nested types | No | Yes | Limited | Yes (struct, list, map) |
| Spec version (2026) | Static | RFC 7946 | 1.4 | 1.1.0 |

### 2.2 GeoParquet Specification Deep Dive

GeoParquet stores geometry as a WKB (Well-Known Binary) column with metadata in the Parquet file footer:

```json
{
  "version": "1.1.0",
  "primary_column": "geometry",
  "columns": {
    "geometry": {
      "encoding": "WKB",
      "geometry_types": ["Polygon", "MultiPolygon"],
      "crs": { ... },          // projjson
      "bbox": [minx, miny, maxx, maxy],
      "covering": {
        "bbox": {
          "xmin": ["bbox", "xmin"],
          "ymin": ["bbox", "ymin"],
          "xmax": ["bbox", "xmax"],
          "ymax": ["bbox", "ymax"]
        }
      }
    }
  }
}
```

**GeoParquet 1.1** introduced the `covering` field, which stores per-row bounding boxes as native Parquet columns. This enables spatial predicate pushdown -- readers can skip entire row groups whose bounding boxes do not intersect the query window.

### 2.3 Reading and Writing GeoParquet with GeoPandas

```python
import geopandas as gpd

# --- Writing ---
gdf = gpd.read_file("parcels.shp")

# Basic write (uses WKB encoding, Snappy compression)
gdf.to_parquet("parcels.parquet")

# Production write with full control
gdf.to_parquet(
    "parcels.parquet",
    compression="zstd",           # Better ratio than Snappy
    write_covering_bbox=True,     # GeoParquet 1.1 covering
    schema_version="1.1.0",
    index=False,                  # Do not serialize pandas index
    row_group_size=100_000,       # Tune for your query patterns
)

# --- Reading ---
# Full read
gdf = gpd.read_parquet("parcels.parquet")

# Column pruning -- only load what you need
gdf = gpd.read_parquet("parcels.parquet", columns=["parcel_id", "area_sqm", "geometry"])

# Bounding box filter (uses covering metadata for pushdown)
from shapely.geometry import box
bbox = box(-122.5, 37.7, -122.3, 37.9)
gdf = gpd.read_parquet("parcels.parquet", bbox=bbox)

# Read from cloud storage (S3, GCS, Azure)
gdf = gpd.read_parquet("s3://my-bucket/parcels.parquet", storage_options={"anon": True})
```

### 2.4 GeoParquet with DuckDB

```sql
-- DuckDB reads GeoParquet natively after loading the spatial extension
INSTALL spatial; LOAD spatial;

-- Read a remote GeoParquet file
SELECT * FROM 'https://data.source.coop/example/buildings.parquet' LIMIT 10;

-- Spatial filter using covering pushdown
SELECT *
FROM read_parquet('s3://bucket/parcels.parquet')
WHERE bbox.xmin >= -122.5
  AND bbox.xmax <= -122.3
  AND bbox.ymin >= 37.7
  AND bbox.ymax <= 37.9;

-- Or use ST_ functions after casting
SELECT parcel_id, ST_Area(ST_GeomFromWKB(geometry)) AS area
FROM read_parquet('parcels.parquet')
WHERE ST_Intersects(
    ST_GeomFromWKB(geometry),
    ST_GeomFromText('POLYGON((-122.5 37.7, -122.3 37.7, -122.3 37.9, -122.5 37.9, -122.5 37.7))')
);
```

### 2.5 GeoArrow: Zero-Copy Geometry in Memory

GeoArrow defines Arrow-native encodings for geometry (point as struct of x/y, linestring as list of points, etc.) eliminating WKB serialization overhead.

```python
import pyarrow as pa
import pyarrow.parquet as pq
import geoarrow.pyarrow as ga

# Read GeoParquet into an Arrow table with GeoArrow extension types
table = pq.read_table("parcels.parquet")

# Convert WKB column to native GeoArrow
geo_col = ga.as_geoarrow(table.column("geometry"))
print(geo_col.type)  # geoarrow.polygon

# Zero-copy exchange with DuckDB
import duckdb
conn = duckdb.connect()
conn.execute("INSTALL spatial; LOAD spatial;")
result = conn.execute("SELECT * FROM arrow_table WHERE ST_Area(geometry) > 1000",
                       {"arrow_table": table})

# Roundtrip back to Arrow
arrow_result = result.fetch_arrow_table()
```

### 2.6 Partitioned GeoParquet (Hive-Style)

For datasets exceeding 10 GB, partitioning is essential:

```python
import geopandas as gpd
import pyarrow as pa
import pyarrow.parquet as pq
from shapely import box
import numpy as np

gdf = gpd.read_parquet("large_dataset.parquet")

# Add partition keys (e.g., country code, admin level, or spatial grid)
gdf["h3_partition"] = gdf.geometry.apply(
    lambda g: h3.latlng_to_cell(g.centroid.y, g.centroid.x, 3)
)

# Write partitioned dataset
table = pa.Table.from_pandas(gdf)
pq.write_to_dataset(
    table,
    root_path="partitioned_data/",
    partition_cols=["h3_partition"],
    compression="zstd",
)

# Read specific partition
gdf_partition = gpd.read_parquet("partitioned_data/h3_partition=831e5afffffffff/")
```

### 2.7 Benchmark: Format Comparison

Test: 10 million polygons, 15 attribute columns, SSD storage, Python 3.12.

| Metric | Shapefile | GeoJSON | GeoPackage | GeoParquet (Snappy) | GeoParquet (Zstd) |
|--------|-----------|---------|------------|--------------------|--------------------|
| File size (GB) | 4.2 | 8.7 | 3.1 | 1.4 | 1.1 |
| Write time (s) | 68 | 142 | 55 | 22 | 28 |
| Full read (s) | 45 | 110 | 38 | 12 | 13 |
| Bbox filter read (s) | 45 (full scan) | 110 (full scan) | 4.2 | 0.8 | 0.9 |
| Column subset read (s) | 45 (full scan) | 110 (full scan) | 36 | 3.5 | 3.7 |
| S3 remote bbox filter | N/A | ~120 | N/A | 2.1 | 2.4 |

> Benchmarks run on M2 MacBook Pro, 32 GB RAM. GeoParquet with covering metadata (1.1) enables 50x speedup for spatial filtering vs Shapefile.

---

## 3. DuckDB Spatial

> See also: [../tools/spatial-databases.md](../tools/spatial-databases.md) for PostGIS and other spatial databases.

### 3.1 Why DuckDB for Geospatial

DuckDB is an in-process OLAP database -- think "SQLite for analytics." Its spatial extension turns it into a powerful geospatial query engine that can read Parquet, CSV, JSON, Shapefile, GeoJSON, GeoPackage, and FlatGeobuf directly, with no data loading step.

Key advantages for GIS:
- **Zero-ETL**: Query files in place, including remote (S3/HTTPS) files
- **Columnar engine**: Predicate pushdown, vectorized execution
- **Spatial extension**: 100+ ST_ functions (GEOS-backed)
- **H3 extension**: Hexagonal hierarchical indexing
- **Runs everywhere**: CLI, Python, R, Node.js, WASM (browser)
- **Memory-efficient**: Spills to disk for datasets exceeding RAM

### 3.2 Installation and Setup

```python
import duckdb

conn = duckdb.connect()  # In-memory database

# Install and load extensions
conn.execute("INSTALL spatial; LOAD spatial;")
conn.execute("INSTALL httpfs; LOAD httpfs;")      # For S3/HTTPS
conn.execute("INSTALL h3 FROM community; LOAD h3;")  # H3 indexing

# Configure S3 access (optional)
conn.execute("""
    SET s3_region = 'us-west-2';
    SET s3_access_key_id = 'AKIA...';
    SET s3_secret_access_key = '...';
""")

# Or use anonymous access for public data
conn.execute("SET s3_url_style = 'path';")
```

```bash
# CLI usage
duckdb -c "INSTALL spatial; INSTALL httpfs;"
duckdb -c "
    LOAD spatial; LOAD httpfs;
    SELECT ST_AsText(geom), name
    FROM ST_Read('https://example.com/places.geojson')
    LIMIT 5;
"
```

### 3.3 Reading Every Format

```sql
LOAD spatial;

-- GeoPackage
SELECT * FROM ST_Read('data.gpkg', layer='buildings');

-- Shapefile
SELECT * FROM ST_Read('parcels.shp');

-- GeoJSON
SELECT * FROM ST_Read('points.geojson');

-- FlatGeobuf
SELECT * FROM ST_Read('roads.fgb');

-- GeoParquet (native, no ST_Read needed)
SELECT * FROM read_parquet('buildings.parquet');

-- CSV with lat/lon
SELECT *, ST_Point(longitude, latitude) AS geom
FROM read_csv('stations.csv');

-- Multiple files with glob
SELECT * FROM read_parquet('tiles/*.parquet');

-- Remote files
SELECT * FROM ST_Read('https://data.source.coop/vida/google-microsoft-osb/geoparquet/by_country/country_iso=USA/*.parquet');

-- Spatial filter on read (FlatGeobuf + GeoPackage support this natively)
SELECT * FROM ST_Read('roads.fgb', spatial_filter=ST_MakeEnvelope(-122.5, 37.7, -122.3, 37.9));
```

### 3.4 Core Spatial SQL Functions

```sql
LOAD spatial;

-- Geometry construction
SELECT ST_Point(-122.4, 37.8) AS pt;
SELECT ST_GeomFromText('POLYGON((0 0, 1 0, 1 1, 0 1, 0 0))') AS poly;
SELECT ST_MakeEnvelope(-122.5, 37.7, -122.3, 37.9) AS bbox;
SELECT ST_Buffer(ST_Point(-122.4, 37.8), 0.01) AS circle;

-- Measurements
SELECT ST_Area(geom) AS area,
       ST_Length(geom) AS perimeter,
       ST_Centroid(geom) AS center,
       ST_Perimeter(geom) AS perim
FROM ST_Read('parcels.gpkg');

-- Spatial relationships
SELECT a.name, b.name
FROM ST_Read('parks.geojson') a, ST_Read('buildings.geojson') b
WHERE ST_Intersects(a.geom, b.geom);

SELECT a.name, b.name, ST_Distance(a.geom, b.geom) AS dist
FROM ST_Read('schools.geojson') a, ST_Read('hospitals.geojson') b
WHERE ST_DWithin(a.geom, b.geom, 0.01);   -- Within ~1 km

-- Spatial joins
SELECT
    tracts.geoid,
    COUNT(pois.id) AS poi_count,
    SUM(pois.revenue) AS total_revenue
FROM read_parquet('census_tracts.parquet') AS tracts
JOIN read_parquet('pois.parquet') AS pois
  ON ST_Contains(
       ST_GeomFromWKB(tracts.geometry),
       ST_GeomFromWKB(pois.geometry)
     )
GROUP BY tracts.geoid;

-- Spatial aggregation
SELECT
    county_name,
    ST_Union_Agg(ST_GeomFromWKB(geometry)) AS merged_geom,
    SUM(population) AS total_pop
FROM read_parquet('blocks.parquet')
GROUP BY county_name;

-- Coordinate transformations
SELECT ST_Transform(geom, 'EPSG:4326', 'EPSG:3857') AS web_mercator
FROM ST_Read('data.geojson');
```

### 3.5 H3 Extension for Hexagonal Indexing

```sql
LOAD spatial; LOAD h3;

-- Point to H3 cell
SELECT h3_latlng_to_cell(37.8, -122.4, 9) AS h3_index;
-- Result: '8928308280fffff'

-- H3 cell to polygon
SELECT h3_cell_to_boundary_wkt('8928308280fffff') AS hex_wkt;

-- Aggregate points to hexagons
SELECT
    h3_latlng_to_cell(ST_Y(geom), ST_X(geom), 8) AS hex,
    COUNT(*) AS count,
    AVG(value) AS avg_value
FROM ST_Read('sensor_readings.geojson')
GROUP BY hex
ORDER BY count DESC;

-- K-ring neighbors (adjacency analysis)
SELECT h3_grid_disk('8928308280fffff', 2) AS neighbors;

-- Compact and uncompact for multi-resolution
SELECT h3_compact(LIST(h3_index)) AS compacted
FROM hex_data;
```

### 3.6 DuckDB + Python Integration

```python
import duckdb
import geopandas as gpd
import pandas as pd

conn = duckdb.connect()
conn.execute("INSTALL spatial; LOAD spatial;")

# Query files directly, get GeoDataFrame
result = conn.execute("""
    SELECT
        name,
        population,
        ST_AsText(ST_GeomFromWKB(geometry)) AS wkt
    FROM read_parquet('cities.parquet')
    WHERE population > 1000000
    ORDER BY population DESC
""").fetchdf()

# Bidirectional: pass GeoDataFrame to DuckDB
gdf = gpd.read_file("neighborhoods.geojson")
conn.register("neighborhoods", gdf)

joined = conn.execute("""
    SELECT n.name, COUNT(*) AS tree_count
    FROM neighborhoods n
    JOIN read_parquet('trees.parquet') t
      ON ST_Contains(n.geometry, ST_GeomFromWKB(t.geometry))
    GROUP BY n.name
""").fetchdf()

# Export results
conn.execute("""
    COPY (
        SELECT * FROM read_parquet('filtered.parquet')
        WHERE ST_Intersects(ST_GeomFromWKB(geometry),
              ST_MakeEnvelope(-74.05, 40.68, -73.85, 40.88))
    ) TO 'nyc_subset.parquet' (FORMAT PARQUET, COMPRESSION ZSTD);
""")
```

### 3.7 DuckDB in the Browser (WASM)

```javascript
// Using duckdb-wasm in a web application
import * as duckdb from '@duckdb/duckdb-wasm';

const db = await duckdb.AsyncDuckDB.create();
await db.open({});
const conn = await db.connect();

// Install spatial extension (WASM build)
await conn.query("INSTALL spatial; LOAD spatial;");
await conn.query("INSTALL httpfs; LOAD httpfs;");

// Query remote GeoParquet from the browser
const result = await conn.query(`
    SELECT name, ST_AsGeoJSON(ST_GeomFromWKB(geometry)) AS geojson
    FROM 'https://data.source.coop/buildings.parquet'
    WHERE bbox.xmin >= -122.5 AND bbox.xmax <= -122.3
    LIMIT 100
`);

// Feed to deck.gl or Mapbox GL JS
const features = result.toArray().map(row => ({
    type: 'Feature',
    properties: { name: row.name },
    geometry: JSON.parse(row.geojson)
}));
```

> Cross-reference: For more on browser-side spatial analytics, see [../js-bindbox/spatial-analysis.md](../js-bindbox/spatial-analysis.md).

---

## 4. Cloud-Optimized GeoTIFF (COG)

> See also: [../data-sources/satellite-imagery.md](../data-sources/satellite-imagery.md) for COG data sources.

### 4.1 What Makes a GeoTIFF "Cloud-Optimized"

A COG is a regular GeoTIFF with two structural requirements:
1. **Internal tiling**: Pixels are stored in tiles (typically 256x256 or 512x512), not strips
2. **Overview pyramids**: Pre-computed reduced-resolution copies are embedded in the file
3. **Byte-range accessible**: The header (IFD) is at the beginning of the file, enabling HTTP range requests to read specific tiles without downloading the whole file

```
Regular GeoTIFF:           Cloud-Optimized GeoTIFF:
+------------------+       +------------------+
| IFD (header)     |       | IFD (header)     |  <-- Read first via HTTP HEAD
| Strip 1          |       | Overview 4x      |
| Strip 2          |       | Overview 2x      |
| Strip 3          |       | Tile (0,0)       |  <-- Read only needed tiles
| ...              |       | Tile (0,1)       |      via HTTP range request
| Strip N          |       | Tile (1,0)       |
+------------------+       | ...              |
                           | Tile (N,M)       |
                           +------------------+
```

### 4.2 Creating COGs

```bash
# From any GeoTIFF -- GDAL is the gold standard
gdal_translate input.tif output_cog.tif \
    -of COG \
    -co COMPRESS=DEFLATE \
    -co PREDICTOR=2 \
    -co BLOCKSIZE=512 \
    -co OVERVIEW_RESAMPLING=AVERAGE \
    -co NUM_THREADS=ALL_CPUS

# Validate a COG
python -m cogeo_mosaic.utils validate output_cog.tif
# Or use rio-cogeo
pip install rio-cogeo
rio cogeo validate output_cog.tif

# Create COG from a NetCDF band
gdal_translate NETCDF:"era5.nc":temperature output_cog.tif \
    -of COG -co COMPRESS=DEFLATE

# Batch convert all TIFFs in a directory
for f in *.tif; do
    gdal_translate "$f" "cog_${f}" -of COG -co COMPRESS=DEFLATE
done
```

```python
# Python: create COG with rasterio
import rasterio
from rasterio.transform import from_bounds
from rasterio.crs import CRS
import numpy as np

data = np.random.rand(4, 10000, 10000).astype(np.float32)
transform = from_bounds(-122.5, 37.0, -121.5, 38.0, 10000, 10000)

profile = {
    "driver": "GTiff",
    "dtype": "float32",
    "width": 10000,
    "height": 10000,
    "count": 4,
    "crs": CRS.from_epsg(4326),
    "transform": transform,
    "tiled": True,
    "blockxsize": 512,
    "blockysize": 512,
    "compress": "deflate",
    "predictor": 2,
}

with rasterio.open("output_cog.tif", "w", **profile) as dst:
    dst.write(data)
    dst.build_overviews([2, 4, 8, 16], rasterio.enums.Resampling.average)
    dst.update_tags(ns="rio_overview", resampling="average")
```

### 4.3 Reading COGs with rioxarray

```python
import rioxarray
import xarray as xr

# Open a remote COG (only reads metadata initially)
ds = rioxarray.open_rasterio(
    "https://sentinel-cogs.s3.us-west-2.amazonaws.com/sentinel-s2-l2a-cogs/36/N/YF/2024/1/S2B_36NYF_20240115_0_L2A/B04.tif",
    overview_level=0,  # Full resolution; use 1, 2, etc. for overviews
    chunks={"x": 512, "y": 512},  # Lazy Dask-backed
)

print(ds)
# <xarray.DataArray (band: 1, y: 10980, x: 10980)>
# dask.array<shape=(1, 10980, 10980), dtype=uint16, chunksize=(1, 512, 512)>

# Windowed read -- only fetches the tiles covering your AOI
aoi = ds.rio.clip_box(minx=36.0, miny=-1.5, maxx=36.5, maxy=-1.0)
data = aoi.compute()  # Actually fetches data now

# Reproject
data_3857 = data.rio.reproject("EPSG:3857")

# Write as COG
data_3857.rio.to_raster("reprojected.tif", driver="COG", compress="deflate")
```

### 4.4 GDAL Virtual Rasters (VRT) for Mosaics

```bash
# Build a VRT from multiple COGs (no data copying)
gdalbuildvrt mosaic.vrt cog_tile_*.tif

# Build VRT from remote files
gdalbuildvrt mosaic.vrt /vsicurl/https://example.com/tile1.tif /vsicurl/https://example.com/tile2.tif

# Read a subset of the mosaic
gdal_translate -projwin -122.5 38.0 -122.0 37.5 mosaic.vrt subset.tif -of COG
```

```python
# Python: virtual mosaic
import rasterio
from rasterio.vrt import WarpedVRT
from rasterio.merge import merge

# Open multiple COGs as a virtual mosaic
files = [
    "s3://bucket/cog_tile_001.tif",
    "s3://bucket/cog_tile_002.tif",
    "s3://bucket/cog_tile_003.tif",
]
datasets = [rasterio.open(f) for f in files]
mosaic, out_transform = merge(datasets, bounds=(-122.5, 37.5, -122.0, 38.0))
```

### 4.5 Performance Tips for COGs

| Tip | Details |
|-----|---------|
| Use DEFLATE or ZSTD compression | DEFLATE is universal; ZSTD is faster but needs GDAL 3.4+ |
| Set PREDICTOR=2 for integers | Horizontal differencing dramatically improves compression |
| Tile size 256 or 512 | 256 = more requests but smaller; 512 = fewer requests |
| Configure GDAL caching | `GDAL_CACHEMAX=512` (MB) for intensive reads |
| Use `/vsicurl/` prefix | Tells GDAL to use HTTP range requests: `/vsicurl/https://...` |
| Set `GDAL_HTTP_MERGE_CONSECUTIVE_RANGES=YES` | Merges adjacent tile requests into one HTTP call |
| Use `CPL_VSIL_CURL_ALLOWED_EXTENSIONS=.tif` | Avoids unnecessary HEAD requests for sidecar files |

```python
import os
# Optimal GDAL environment for cloud reads
os.environ["GDAL_HTTP_MERGE_CONSECUTIVE_RANGES"] = "YES"
os.environ["GDAL_HTTP_MULTIPLEX"] = "YES"
os.environ["GDAL_HTTP_VERSION"] = "2"
os.environ["GDAL_CACHEMAX"] = "512"
os.environ["VSI_CACHE"] = "TRUE"
os.environ["VSI_CACHE_SIZE"] = "536870912"  # 512 MB
os.environ["CPL_VSIL_CURL_ALLOWED_EXTENSIONS"] = ".tif,.vrt"
```

---

## 5. STAC Ecosystem

> See also: [../data-sources/satellite-imagery.md](../data-sources/satellite-imagery.md) for STAC catalog URLs; [../tools/cloud-platforms.md](../tools/cloud-platforms.md) for platform-hosted catalogs.

### 5.1 What is STAC?

SpatioTemporal Asset Catalog (STAC) is a specification for cataloging geospatial data so it can be crawled, searched, and consumed by standard tools. Think of it as "the catalog layer" for cloud-native geospatial.

Components:
- **STAC Item**: A single spatiotemporal asset (e.g., one Sentinel-2 scene)
- **STAC Collection**: A group of related items (e.g., "Sentinel-2 L2A")
- **STAC Catalog**: A top-level container linking to collections
- **STAC API**: A REST API for searching items (filter by bbox, datetime, properties)

### 5.2 Searching STAC Catalogs with pystac-client

```python
from pystac_client import Client
from shapely.geometry import box, mapping

# Connect to a STAC API
catalog = Client.open("https://earth-search.aws.element84.com/v1")

# List available collections
for collection in catalog.get_collections():
    print(f"{collection.id}: {collection.title}")

# Search for Sentinel-2 scenes
search = catalog.search(
    collections=["sentinel-2-l2a"],
    bbox=[-122.5, 37.5, -122.0, 38.0],       # San Francisco area
    datetime="2024-06-01/2024-06-30",
    query={"eo:cloud_cover": {"lt": 20}},      # Less than 20% cloud
    max_items=50,
)

items = search.item_collection()
print(f"Found {len(items)} items")

# Inspect an item
item = items[0]
print(f"ID: {item.id}")
print(f"Date: {item.datetime}")
print(f"Cloud cover: {item.properties['eo:cloud_cover']}%")
print(f"Assets: {list(item.assets.keys())}")
print(f"Red band URL: {item.assets['red'].href}")
```

### 5.3 Loading STAC Items into xarray with stackstac

```python
import stackstac
import pystac_client

catalog = pystac_client.Client.open("https://earth-search.aws.element84.com/v1")

search = catalog.search(
    collections=["sentinel-2-l2a"],
    bbox=[-122.5, 37.5, -122.0, 38.0],
    datetime="2024-06-01/2024-08-31",
    query={"eo:cloud_cover": {"lt": 10}},
)
items = search.item_collection()

# Stack into an xarray DataArray (lazy -- backed by Dask)
stack = stackstac.stack(
    items,
    assets=["red", "green", "blue", "nir"],
    resolution=10,                    # 10m
    bounds_latlon=(-122.5, 37.5, -122.0, 38.0),
    chunksize=2048,                   # Dask chunk size
    epsg=32610,                       # UTM 10N
)

print(stack)
# <xarray.DataArray 'stackstac-...' (time: 12, band: 4, y: 5549, x: 4279)>
# dask.array<shape=(12, 4, 5549, 4279), dtype=float64>

# Compute NDVI across all time steps (lazy)
red = stack.sel(band="red")
nir = stack.sel(band="nir")
ndvi = (nir - red) / (nir + red)

# Temporal median composite
ndvi_median = ndvi.median(dim="time")

# Actually compute (triggers data download)
result = ndvi_median.compute()
```

### 5.4 odc-stac: Open Data Cube Integration

```python
import odc.stac
import pystac_client

catalog = pystac_client.Client.open("https://earth-search.aws.element84.com/v1")
items = catalog.search(
    collections=["sentinel-2-l2a"],
    bbox=[-122.5, 37.5, -122.0, 38.0],
    datetime="2024-07-01/2024-07-31",
    query={"eo:cloud_cover": {"lt": 5}},
).item_collection()

# Load as xarray Dataset (not DataArray)
ds = odc.stac.load(
    items,
    bands=["red", "green", "blue", "nir", "scl"],
    resolution=10,
    bbox=(-122.5, 37.5, -122.0, 38.0),
    crs="EPSG:32610",
    chunks={"x": 2048, "y": 2048},
    groupby="solar_day",  # Group items from same day
)

print(ds)
# <xarray.Dataset>
# Dimensions: (time: 3, y: 5549, x: 4279)
# Data variables: red, green, blue, nir, scl

# Cloud masking using SCL band
cloud_free = ds.where(ds.scl.isin([4, 5, 6, 7]))  # Vegetation, bare, water, snow

# Monthly NDVI
ndvi = (cloud_free.nir - cloud_free.red) / (cloud_free.nir + cloud_free.red)
ndvi_monthly = ndvi.resample(time="1ME").median()
```

### 5.5 STAC Catalog Quick Reference

| Catalog | URL | Key Collections |
|---------|-----|-----------------|
| Earth Search (Element84) | earth-search.aws.element84.com/v1 | Sentinel-2, Landsat, NAIP, Copernicus DEM |
| Microsoft Planetary Computer | planetarycomputer.microsoft.com/api/stac/v1 | Sentinel-2, Landsat, MODIS, ERA5, Aster |
| USGS STAC | landsatlook.usgs.gov/stac-server | Landsat Collection 2 |
| Google Earth Engine STAC | earthengine-stac.storage.googleapis.com/catalog/catalog.json | GEE catalog (static) |
| Copernicus Data Space | catalogue.dataspace.copernicus.eu/stac | Sentinel-1/2/3/5P |
| NASA CMR STAC | cmr.earthdata.nasa.gov/stac | All NASA missions |

---

## 6. xarray for Geospatial

> See also: [Python Geospatial Stack](python-stack.md) for rasterio and other fundamentals.

### 6.1 Why xarray for Geospatial Rasters

xarray provides N-dimensional labeled arrays with metadata. For geospatial rasters, this means:
- Named dimensions: `(time, band, y, x)` instead of cryptic axis numbers
- Coordinate labels: actual lat/lon or projected coordinates on each axis
- Lazy computation via Dask: process terabytes without loading into RAM
- Rich ecosystem: rioxarray for CRS/transform, xarray-spatial for raster analytics

### 6.2 Core xarray + rioxarray Patterns

```python
import xarray as xr
import rioxarray

# Open a multi-band raster
ds = rioxarray.open_rasterio("sentinel2_scene.tif", chunks={"x": 512, "y": 512})
print(ds.rio.crs)        # EPSG:32610
print(ds.rio.bounds())   # (xmin, ymin, xmax, ymax)
print(ds.rio.resolution())  # (10.0, -10.0)

# Clip to a geometry
from shapely.geometry import box
aoi = box(-122.4, 37.7, -122.3, 37.8)
clipped = ds.rio.clip_box(*aoi.bounds)

# Reproject
ds_4326 = ds.rio.reproject("EPSG:4326")

# Write
ds_4326.rio.to_raster("output.tif", driver="COG")
```

### 6.3 Zarr: Cloud-Native N-Dimensional Storage

Zarr is the "COG for data cubes" -- a chunked, compressed, N-dimensional array format designed for cloud storage.

```python
import xarray as xr
import zarr

# Write xarray dataset to Zarr
ds = xr.open_dataset("era5_temperature.nc")
ds = ds.chunk({"time": 100, "latitude": 100, "longitude": 100})
ds.to_zarr("s3://bucket/era5_temp.zarr", mode="w", consolidated=True)

# Read from cloud (lazy)
ds = xr.open_zarr("s3://bucket/era5_temp.zarr", chunks="auto")

# Temporal selection -- only reads needed chunks
summer = ds.sel(time=slice("2024-06-01", "2024-08-31"))
mean_temp = summer.mean(dim="time")
result = mean_temp.compute()

# Append new time steps
new_data = xr.open_dataset("era5_september.nc")
new_data.to_zarr("s3://bucket/era5_temp.zarr", mode="a", append_dim="time")
```

### 6.4 Parallel Computation with Dask

```python
import xarray as xr
import dask
from dask.distributed import Client

# Start a Dask cluster
client = Client(n_workers=8, threads_per_worker=2, memory_limit="4GB")
print(client.dashboard_link)  # http://localhost:8787

# Open a large Zarr store with Dask backing
ds = xr.open_zarr("s3://bucket/global_temperature.zarr", chunks={"time": 50, "lat": 500, "lon": 500})

# Compute monthly anomalies -- runs in parallel across chunks
climatology = ds.groupby("time.month").mean("time")
anomalies = ds.groupby("time.month") - climatology

# Persist intermediate results in cluster memory
anomalies = anomalies.persist()

# Compute spatial statistics
zonal_mean = anomalies.mean(dim=["lat", "lon"]).compute()
```

### 6.5 xarray-spatial for Raster Analytics

```python
import xarray as xr
import xrspatial

# Load DEM
dem = rioxarray.open_rasterio("dem_30m.tif").squeeze()

# Terrain analysis (GPU-accelerated if CuPy available)
slope = xrspatial.slope(dem)
aspect = xrspatial.aspect(dem)
hillshade = xrspatial.hillshade(dem, azimuth=315, angle_altitude=45)
curvature = xrspatial.curvature(dem)

# Focal statistics
from xrspatial.focal import mean as focal_mean
smoothed = focal_mean(dem, kernel=xrspatial.convolution.circle_kernel(3, 3, 1))

# Zonal statistics
import geopandas as gpd
zones = gpd.read_file("watersheds.geojson")
from xrspatial.zonal import stats as zonal_stats
results = zonal_stats(zones, dem)  # Returns mean, min, max, std per zone

# Proximity analysis
from xrspatial.proximity import proximity
distance_to_water = proximity(water_mask)
```

### 6.6 xarray Benchmark: Zarr vs NetCDF vs COG

Test: Global 0.25-degree temperature dataset, 365 time steps, 721x1440 grid.

| Operation | NetCDF4 (local) | Zarr (local) | Zarr (S3) | COG stack (S3) |
|-----------|-----------------|--------------|-----------|----------------|
| Open metadata | 0.1s | 0.05s | 0.8s | 2.1s |
| Read single timestep | 0.3s | 0.2s | 1.2s | 1.5s |
| Read 1-year time series, one point | 1.8s | 0.4s | 1.5s | 12.3s (365 files) |
| Compute annual mean | 12s | 8s | 45s | N/A |
| Spatial subset (100x100 box) | 11s (full scan) | 0.6s | 2.1s | 3.2s |

> Zarr excels at chunked access patterns. COGs are better for single-time-step spatial queries. NetCDF remains viable for local, moderate-size datasets.

---

## 7. Apache Sedona

> See also: [../tools/cloud-platforms.md](../tools/cloud-platforms.md) for managed Spark platforms.

### 7.1 When to Use Sedona

Apache Sedona (formerly GeoSpark) is the go-to tool when your data exceeds what a single machine can handle. Use it when:
- Vector datasets exceed 50-100 GB
- You need spatial joins between two large datasets
- You already have a Spark cluster
- You need raster + vector combined processing at scale

### 7.2 Setup and Initialization

```python
# pip install apache-sedona[spark]

from sedona.spark import SedonaContext

config = (
    SedonaContext.builder()
    .master("local[*]")  # or "spark://cluster:7077"
    .appName("GeoAnalysis")
    .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
    .config("spark.kryo.registrator", "org.apache.sedona.core.serde.SedonaKryoRegistrator")
    .config("spark.jars.packages", "org.apache.sedona:sedona-spark-shaded-3.5_2.12:1.6.1,"
            "org.datasyslab:geotools-wrapper:1.6.1-28.2")
    .getOrCreate()
)

sedona = SedonaContext.create(config)
```

### 7.3 Spatial SQL at Scale

```python
# Read GeoParquet (distributed)
sedona.read.format("geoparquet").load("s3://bucket/buildings/").createOrReplaceTempView("buildings")
sedona.read.format("geoparquet").load("s3://bucket/census_tracts/").createOrReplaceTempView("tracts")

# Spatial join: count buildings per tract (distributed, uses spatial partitioning)
result = sedona.sql("""
    SELECT
        t.geoid,
        t.name,
        COUNT(b.id) AS building_count,
        SUM(ST_Area(ST_Transform(b.geometry, 'EPSG:4326', 'EPSG:3857'))) AS total_building_area
    FROM tracts t
    JOIN buildings b
      ON ST_Contains(t.geometry, b.geometry)
    GROUP BY t.geoid, t.name
""")

result.write.format("geoparquet").save("s3://bucket/results/buildings_per_tract/")

# KNN: nearest 5 hospitals for each school
sedona.sql("""
    SELECT s.name AS school, h.name AS hospital, ST_Distance(s.geometry, h.geometry) AS dist
    FROM schools s
    CROSS JOIN LATERAL (
        SELECT h.name, h.geometry
        FROM hospitals h
        ORDER BY ST_Distance(s.geometry, h.geometry)
        LIMIT 5
    ) h
""")

# Buffer + dissolve at continental scale
sedona.sql("""
    SELECT
        land_use_type,
        ST_Union_Aggr(ST_Buffer(geometry, 0.001)) AS buffered_union
    FROM read.format("geoparquet").load("s3://bucket/landuse/")
    GROUP BY land_use_type
""")
```

### 7.4 Raster Processing with Sedona

```python
# Read raster files
raster_df = sedona.read.format("binaryFile") \
    .option("pathGlobFilter", "*.tif") \
    .load("s3://bucket/dem_tiles/")

# Register raster UDFs
sedona.sql("""
    SELECT RS_Value(rast, ST_Point(-122.4, 37.8)) AS elevation
    FROM raster_table
""")

# Raster-vector overlay
sedona.sql("""
    SELECT
        p.parcel_id,
        RS_ZonalStats(r.rast, p.geometry, 'mean') AS mean_elevation,
        RS_ZonalStats(r.rast, p.geometry, 'max') AS max_elevation
    FROM parcels p, raster_tiles r
    WHERE ST_Intersects(p.geometry, RS_Envelope(r.rast))
""")
```

---

## 8. Dask-GeoPandas

### 8.1 When to Use Dask-GeoPandas

Dask-GeoPandas extends GeoPandas to work with datasets that do not fit in memory, using Dask's lazy evaluation and parallel execution. Use it when:
- Your dataset is 2-50 GB (above GeoPandas, below Sedona)
- You want the familiar GeoPandas API
- You need parallel spatial operations on a single multi-core machine
- You want out-of-core spatial joins

### 8.2 Core Usage

```python
import dask_geopandas
import geopandas as gpd

# Read large GeoParquet as Dask-GeoDataFrame
ddf = dask_geopandas.read_parquet("large_buildings.parquet", npartitions=16)
print(ddf)  # Dask GeoDataFrame with 16 partitions

# Familiar GeoPandas API, but lazy
ddf["area"] = ddf.geometry.area
ddf["centroid"] = ddf.geometry.centroid

# Filter (predicate pushdown for Parquet)
large_buildings = ddf[ddf["area"] > 500]

# Spatial dissolve
dissolved = ddf.dissolve(by="county", aggfunc="sum")

# Compute to get a regular GeoDataFrame
result = large_buildings.compute()

# Or write back to Parquet without loading into memory
large_buildings.to_parquet("filtered_buildings/")
```

### 8.3 Spatial Partitioning

Spatial partitioning ensures that nearby features are in the same partition, dramatically speeding up spatial joins.

```python
import dask_geopandas

ddf = dask_geopandas.read_parquet("buildings.parquet")

# Repartition spatially using Hilbert curve
ddf_spatial = ddf.spatial_shuffle(by="hilbert", npartitions=64)

# Now spatial joins are partition-aligned
tracts = dask_geopandas.read_parquet("tracts.parquet")
tracts_spatial = tracts.spatial_shuffle(by="hilbert", npartitions=64)

# Spatial join (much faster with spatial partitioning)
joined = dask_geopandas.sjoin(ddf_spatial, tracts_spatial, predicate="within")
result = joined.groupby("tract_id").size().compute()
```

### 8.4 Out-of-Core Processing Pipeline

```python
import dask_geopandas
from dask.distributed import Client

client = Client(n_workers=8, memory_limit="4GB")

# Process 30 GB of OpenStreetMap data
ddf = dask_geopandas.read_parquet("s3://bucket/osm_buildings_global.parquet")

# Chain operations (all lazy)
pipeline = (
    ddf
    .assign(area_sqm=lambda d: d.geometry.to_crs(epsg=3857).area)
    .query("area_sqm > 100 and area_sqm < 100000")
    .assign(h3_cell=lambda d: d.geometry.centroid.apply(
        lambda p: h3.latlng_to_cell(p.y, p.x, 8), meta=("h3_cell", "str")
    ))
    .groupby("h3_cell")
    .agg({"area_sqm": ["count", "mean", "sum"]})
)

# Execute and save
pipeline.to_parquet("s3://bucket/building_stats_h3/")
```

---

## 9. FlatGeobuf

> See also: [../js-bindbox/data-formats-loading.md](../js-bindbox/data-formats-loading.md) for browser-side FlatGeobuf.

### 9.1 What FlatGeobuf Does Best

FlatGeobuf is a binary format optimized for **streaming spatial queries over HTTP**. Its killer feature: the built-in packed Hilbert R-tree index enables HTTP range requests that return only features intersecting a bounding box -- no server-side processing required.

### 9.2 FlatGeobuf vs GeoParquet

| Feature | FlatGeobuf | GeoParquet |
|---------|-----------|------------|
| Primary use case | Streaming to web clients | Analytical queries |
| Spatial index | Packed Hilbert R-tree | Bbox covering (1.1) |
| HTTP spatial filter | Excellent (precise) | Good (row-group level) |
| Column pruning | No (row-oriented) | Yes (columnar) |
| Compression | None | Snappy/Zstd |
| Attribute filtering | No | Yes (predicate pushdown) |
| Append support | No | Via partitioning |
| Browser support | Native (flatgeobuf.js) | Via duckdb-wasm |
| Best for | Serve features to maps | Analyze large datasets |

**Rule of thumb**: Use FlatGeobuf when the consumer is a web map. Use GeoParquet when the consumer is an analytics engine.

### 9.3 Creating and Querying FlatGeobuf

```python
import geopandas as gpd

# Write
gdf = gpd.read_file("buildings.shp")
gdf.to_file("buildings.fgb", driver="FlatGeobuf")

# Read with spatial filter (uses the built-in R-tree index)
from shapely.geometry import box
bbox = box(-122.5, 37.7, -122.3, 37.9)
gdf_filtered = gpd.read_file("buildings.fgb", bbox=bbox)

# Read from HTTP (streaming spatial filter)
gdf_remote = gpd.read_file(
    "https://example.com/buildings.fgb",
    bbox=(-122.5, 37.7, -122.3, 37.9)
)
```

```javascript
// Browser: flatgeobuf + Leaflet
import { deserialize } from 'flatgeobuf/lib/mjs/geojson.js';

const bounds = map.getBounds();
const rect = {
    minX: bounds.getWest(), minY: bounds.getSouth(),
    maxX: bounds.getEast(), maxY: bounds.getNorth()
};

const iter = flatgeobuf.deserialize(
    'https://example.com/buildings.fgb',
    rect  // Spatial filter -- only fetches intersecting features
);

for await (const feature of iter) {
    L.geoJSON(feature).addTo(map);
}
```

### 9.4 DuckDB + FlatGeobuf

```sql
LOAD spatial;

-- Read with spatial filter
SELECT name, type, geom
FROM ST_Read('buildings.fgb',
    spatial_filter=ST_MakeEnvelope(-122.5, 37.7, -122.3, 37.9)
);

-- Convert FlatGeobuf to GeoParquet
COPY (SELECT * FROM ST_Read('input.fgb'))
TO 'output.parquet' (FORMAT PARQUET, COMPRESSION ZSTD);
```

---

## 10. Google Earth Engine

> See also: [../data-sources/satellite-imagery.md](../data-sources/satellite-imagery.md); [../tools/cloud-platforms.md](../tools/cloud-platforms.md).

### 10.1 Overview and Key Concepts

Google Earth Engine (GEE) is a planetary-scale geospatial analysis platform with a multi-petabyte catalog. It is **server-side**: your Python code defines a computation graph that GEE executes on Google's infrastructure.

**Critical concept: server-side vs client-side.**

```python
import ee
ee.Initialize(project='my-gee-project')

# SERVER-SIDE: This runs on Google's servers
image = ee.Image("COPERNICUS/S2_SR_HARMONIZED/20240701T185919_20240701T190743_T10SEG")
ndvi = image.normalizedDifference(["B8", "B4"]).rename("NDVI")

# CLIENT-SIDE: This triggers actual computation and data transfer
# AVOID calling .getInfo() on large results -- it pulls data to your machine
local_value = ndvi.reduceRegion(
    reducer=ee.Reducer.mean(),
    geometry=ee.Geometry.Point(-122.4, 37.8).buffer(1000),
    scale=10
).getInfo()
print(local_value)  # {'NDVI': 0.42}
```

### 10.2 Common Workflows

```python
import ee
import geemap

ee.Initialize(project='my-gee-project')

# --- Cloud-free composite ---
collection = (
    ee.ImageCollection("COPERNICUS/S2_SR_HARMONIZED")
    .filterDate("2024-06-01", "2024-08-31")
    .filterBounds(ee.Geometry.Rectangle([-122.5, 37.5, -122.0, 38.0]))
    .filter(ee.Filter.lt("CLOUDY_PIXEL_PERCENTAGE", 20))
)

composite = collection.median()

# --- NDVI time series ---
def add_ndvi(image):
    ndvi = image.normalizedDifference(["B8", "B4"]).rename("NDVI")
    return image.addBands(ndvi)

ndvi_collection = collection.map(add_ndvi).select("NDVI")

# Chart (via geemap)
Map = geemap.Map()
Map.addLayer(composite, {"bands": ["B4", "B3", "B2"], "min": 0, "max": 3000}, "RGB")
Map.addLayer(ndvi_collection.median(), {"min": 0, "max": 0.8, "palette": ["red", "yellow", "green"]}, "NDVI")
Map

# --- Land cover classification ---
training = composite.select(["B2","B3","B4","B8","B11","B12"]).sampleRegions(
    collection=training_points,
    properties=["land_cover"],
    scale=10
)
classifier = ee.Classifier.smileRandomForest(100).train(training, "land_cover")
classified = composite.select(["B2","B3","B4","B8","B11","B12"]).classify(classifier)

# --- Export results ---
task = ee.batch.Export.image.toDrive(
    image=classified,
    description="land_cover_2024",
    scale=10,
    region=ee.Geometry.Rectangle([-122.5, 37.5, -122.0, 38.0]),
    maxPixels=1e10,
    fileFormat="GeoTIFF",
)
task.start()

# Export to GCS (better for large exports)
task = ee.batch.Export.image.toCloudStorage(
    image=classified,
    description="land_cover_2024",
    bucket="my-bucket",
    scale=10,
    region=ee.Geometry.Rectangle([-122.5, 37.5, -122.0, 38.0]),
    maxPixels=1e10,
    fileFormat="GeoTIFF",
    formatOptions={"cloudOptimized": True},  # Export as COG
)
task.start()
```

### 10.3 Cost Considerations

| Tier | Compute | Storage | Notes |
|------|---------|---------|-------|
| Free (noncommercial) | Unlimited (fair use) | 250 GB Assets | Requires GCP project |
| Free (academic) | Unlimited (fair use) | 250 GB Assets | .edu email required |
| Commercial | Pay-per-use (EECU-hours) | Standard GCS rates | Starts ~$1/EECU-hour |
| Batch export | Free | GCS/Drive rates apply | Queue-based, can be slow |

**Cost optimization tips:**
- Use `scale` parameter matching your output -- do not process at 10m if you need 30m output
- Use `bestEffort=True` in `reduceRegion()` to avoid timeout on large regions
- Export large results to GCS as COG rather than calling `.getInfo()`
- Use `ee.batch.Export` for anything larger than a single tile

---

## 11. Microsoft Planetary Computer

> See also: [../tools/cloud-platforms.md](../tools/cloud-platforms.md).

### 11.1 Overview

Microsoft Planetary Computer (MPC) provides:
1. A massive STAC catalog of environmental datasets (Sentinel-2, Landsat, MODIS, ERA5, and more)
2. A free managed JupyterHub with Dask Gateway for scaling
3. All data stored as cloud-native formats (COG, Zarr, GeoParquet)

### 11.2 Accessing the Catalog

```python
import planetary_computer
import pystac_client

catalog = pystac_client.Client.open(
    "https://planetarycomputer.microsoft.com/api/stac/v1",
    modifier=planetary_computer.sign_inplace,  # Signs URLs for access
)

# Search Sentinel-2
search = catalog.search(
    collections=["sentinel-2-l2a"],
    bbox=[-122.5, 37.5, -122.0, 38.0],
    datetime="2024-07-01/2024-07-31",
    query={"eo:cloud_cover": {"lt": 10}},
)

items = search.item_collection()
print(f"Found {len(items)} items")

# Each item's assets are signed (time-limited URLs)
item = items[0]
print(item.assets["B04"].href)  # Signed Azure Blob URL
```

### 11.3 Scaling with Dask Gateway

```python
# On the Planetary Computer Hub (JupyterHub)
from dask_gateway import GatewayCluster

cluster = GatewayCluster()
cluster.scale(16)  # 16 workers (free!)
client = cluster.get_client()
print(client.dashboard_link)

# Now load and process data at scale
import odc.stac

ds = odc.stac.load(
    items,
    bands=["B04", "B08"],
    resolution=10,
    chunks={"x": 2048, "y": 2048},
)

ndvi = (ds.B08 - ds.B04) / (ds.B08 + ds.B04)
result = ndvi.mean(dim="time").compute()  # Runs on Dask cluster
```

### 11.4 Available Collections (Highlights)

| Collection | Resolution | Temporal | Format |
|-----------|-----------|----------|--------|
| sentinel-2-l2a | 10-60m | 5-day | COG |
| landsat-c2-l2 | 30m | 16-day | COG |
| modis-*-061 | 250m-1km | Daily | COG/HDF |
| era5-pds | 0.25deg | Hourly | Zarr |
| cop-dem-glo-30 | 30m | Static | COG |
| io-lulc-9-class | 10m | Annual | COG |
| ms-buildings | Vector | Static | GeoParquet |
| alos-dem | 30m | Static | COG |

### 11.5 End-to-End MPC Workflow: NDVI Anomaly Detection

```python
import planetary_computer
import pystac_client
import odc.stac
import xarray as xr
import numpy as np

catalog = pystac_client.Client.open(
    "https://planetarycomputer.microsoft.com/api/stac/v1",
    modifier=planetary_computer.sign_inplace,
)

bbox = [-122.5, 37.5, -122.0, 38.0]

# Get 3 years of summer scenes
items_list = []
for year in [2022, 2023, 2024]:
    search = catalog.search(
        collections=["sentinel-2-l2a"],
        bbox=bbox,
        datetime=f"{year}-06-01/{year}-08-31",
        query={"eo:cloud_cover": {"lt": 10}},
    )
    items_list.extend(search.item_collection())

# Load into xarray
ds = odc.stac.load(
    items_list,
    bands=["B04", "B08", "SCL"],
    resolution=10,
    bbox=bbox,
    chunks={"x": 2048, "y": 2048},
    groupby="solar_day",
)

# Cloud mask
cloud_free = ds.where(ds.SCL.isin([4, 5, 6, 7]))

# NDVI
ndvi = (cloud_free.B08 - cloud_free.B04) / (cloud_free.B08 + cloud_free.B04)

# Annual medians
annual_ndvi = ndvi.resample(time="1YE").median()

# Anomaly: 2024 vs 2022-2023 baseline
baseline = annual_ndvi.sel(time=slice("2022", "2023")).mean(dim="time")
current = annual_ndvi.sel(time="2024").squeeze()
anomaly = current - baseline

result = anomaly.compute()
result.rio.to_raster("ndvi_anomaly_2024.tif", driver="COG")
```

---

## 12. Overture Maps

> See also: [../data-sources/vector-data.md](../data-sources/vector-data.md) for other vector data sources.

### 12.1 What is Overture Maps

Overture Maps Foundation (backed by Amazon, Meta, Microsoft, TomTom) publishes open map data as GeoParquet, organized into themes: buildings, places, transportation, divisions, addresses, base, and land-use. The data is released quarterly and covers the entire globe.

Key facts:
- ~2.5 billion features total (as of 2025)
- ~1.5 billion building footprints
- ~60 million places (POI)
- All data available as partitioned GeoParquet on Azure and AWS
- Optimized for DuckDB queries

### 12.2 Querying with DuckDB

```sql
-- Setup
INSTALL spatial; LOAD spatial;
INSTALL httpfs; LOAD httpfs;

-- Configure S3 for Overture data
SET s3_region = 'us-west-2';

-- ================================================================
-- Buildings: Query building footprints by bounding box
-- ================================================================
SELECT
    id,
    names.primary AS name,
    class,
    subtype,
    height,
    num_floors,
    ST_GeomFromWKB(geometry) AS geom
FROM read_parquet('s3://overturemaps-us-west-2/release/2024-12-18.0/theme=buildings/type=building/*')
WHERE bbox.xmin >= -122.45
  AND bbox.xmax <= -122.35
  AND bbox.ymin >= 37.75
  AND bbox.ymax <= 37.85;

-- ================================================================
-- Places (POI): Find restaurants in an area
-- ================================================================
SELECT
    id,
    names.primary AS name,
    categories.primary AS category,
    addresses[1].freeform AS address,
    confidence,
    ST_GeomFromWKB(geometry) AS geom
FROM read_parquet('s3://overturemaps-us-west-2/release/2024-12-18.0/theme=places/type=place/*')
WHERE bbox.xmin >= -122.45
  AND bbox.xmax <= -122.35
  AND bbox.ymin >= 37.75
  AND bbox.ymax <= 37.85
  AND categories.primary = 'restaurant'
ORDER BY confidence DESC
LIMIT 100;

-- ================================================================
-- Transportation: Road segments
-- ================================================================
SELECT
    id,
    names.primary AS name,
    class,
    subclass,
    ST_GeomFromWKB(geometry) AS geom,
    road_surface,
    speed_limits
FROM read_parquet('s3://overturemaps-us-west-2/release/2024-12-18.0/theme=transportation/type=segment/*')
WHERE bbox.xmin >= -122.45
  AND bbox.xmax <= -122.35
  AND bbox.ymin >= 37.75
  AND bbox.ymax <= 37.85
  AND class IN ('primary', 'secondary', 'tertiary');

-- ================================================================
-- Export subset to local GeoParquet
-- ================================================================
COPY (
    SELECT *
    FROM read_parquet('s3://overturemaps-us-west-2/release/2024-12-18.0/theme=buildings/type=building/*')
    WHERE bbox.xmin >= -122.55
      AND bbox.xmax <= -122.25
      AND bbox.ymin >= 37.65
      AND bbox.ymax <= 37.95
) TO 'sf_buildings.parquet' (FORMAT PARQUET, COMPRESSION ZSTD);
```

### 12.3 Python Workflow: Buildings Analysis

```python
import duckdb
import geopandas as gpd

conn = duckdb.connect()
conn.execute("INSTALL spatial; LOAD spatial; INSTALL httpfs; LOAD httpfs;")
conn.execute("SET s3_region = 'us-west-2';")

# Count buildings by class in San Francisco
stats = conn.execute("""
    SELECT
        class,
        COUNT(*) AS count,
        AVG(height) AS avg_height,
        AVG(num_floors) AS avg_floors
    FROM read_parquet('s3://overturemaps-us-west-2/release/2024-12-18.0/theme=buildings/type=building/*')
    WHERE bbox.xmin >= -122.55 AND bbox.xmax <= -122.35
      AND bbox.ymin >= 37.70 AND bbox.ymax <= 37.85
    GROUP BY class
    ORDER BY count DESC
""").fetchdf()

print(stats)
# Output:
#            class   count  avg_height  avg_floors
# 0   residential  125432        8.2         2.4
# 1    commercial   23456       24.5         6.1
# 2    industrial    8765       12.3         2.8
# ...

# Load building footprints as GeoDataFrame for mapping
gdf = conn.execute("""
    SELECT
        id,
        names.primary AS name,
        class,
        height,
        ST_AsText(ST_GeomFromWKB(geometry)) AS wkt
    FROM read_parquet('s3://overturemaps-us-west-2/release/2024-12-18.0/theme=buildings/type=building/*')
    WHERE bbox.xmin >= -122.42 AND bbox.xmax <= -122.40
      AND bbox.ymin >= 37.78 AND bbox.ymax <= 37.80
      AND height IS NOT NULL
""").fetchdf()

gdf = gpd.GeoDataFrame(gdf, geometry=gpd.GeoSeries.from_wkt(gdf.wkt), crs="EPSG:4326")
gdf = gdf.drop(columns=["wkt"])
```

### 12.4 Overture Maps CLI

```bash
# Install the official CLI
pip install overturemaps

# Download buildings for a bounding box
overturemaps download \
    --bbox=-122.5,37.7,-122.3,37.9 \
    -f geoparquet \
    --type=building \
    -o sf_buildings.parquet

# Download places (POI)
overturemaps download \
    --bbox=-122.5,37.7,-122.3,37.9 \
    -f geojson \
    --type=place \
    -o sf_places.geojson

# Download as GeoJSON Lines (streaming-friendly)
overturemaps download \
    --bbox=-74.05,40.68,-73.85,40.88 \
    -f geojsonseq \
    --type=building \
    -o nyc_buildings.geojsonl
```

### 12.5 Overture Data Themes Reference

| Theme | Types | Feature Count (2025) | Key Properties |
|-------|-------|---------------------|---------------|
| buildings | building | ~1.5B | class, height, num_floors, roof shape |
| places | place | ~60M | categories, confidence, websites, phones |
| transportation | segment, connector | ~500M | class, speed_limits, access, road_surface |
| divisions | division, division_area, boundary | ~500K | country, region, subtype |
| addresses | address | ~200M | freeform, locality, postcode, country |
| base | water, infrastructure, land | ~100M | class, subtype, names |
| land_use | land_use | ~50M | class, subtype |

---

## 13. Data Pipeline Patterns

> See also: [../tools/etl-data-engineering.md](../tools/etl-data-engineering.md); [../tools/cli-tools.md](../tools/cli-tools.md) for GDAL/OGR.

### 13.1 ETL with GDAL + Python

GDAL/OGR remains the Swiss Army knife for format conversion and spatial ETL.

```bash
# Convert Shapefile to GeoParquet
ogr2ogr -f Parquet output.parquet input.shp \
    -lco COMPRESSION=ZSTD \
    -lco WRITE_COVERING_BBOX=YES \
    -lco ROW_GROUP_SIZE=100000

# Convert GeoJSON to GeoParquet with reprojection
ogr2ogr -f Parquet output.parquet input.geojson \
    -t_srs EPSG:4326 \
    -lco COMPRESSION=ZSTD

# Spatial filter during conversion
ogr2ogr -f Parquet sf_parcels.parquet parcels.gpkg \
    -spat -122.5 37.7 -122.3 37.9

# SQL-based extraction
ogr2ogr -f Parquet commercial_buildings.parquet buildings.gpkg \
    -sql "SELECT * FROM buildings WHERE land_use = 'commercial' AND area > 500"

# Merge multiple files
ogr2ogr -f Parquet merged.parquet tile_001.gpkg
ogr2ogr -f Parquet -append merged.parquet tile_002.gpkg
ogr2ogr -f Parquet -append merged.parquet tile_003.gpkg

# Batch conversion with GNU parallel
find . -name "*.shp" | parallel -j8 \
    'ogr2ogr -f Parquet {.}.parquet {} -lco COMPRESSION=ZSTD'
```

```python
# Python ETL: Download, transform, export
import geopandas as gpd
import duckdb
from pathlib import Path

def etl_parcels(input_url: str, output_path: str, bbox: tuple):
    """Download, filter, clean, and export parcel data."""
    # Extract
    gdf = gpd.read_file(input_url, bbox=bbox)

    # Transform
    gdf = gdf.to_crs(epsg=4326)
    gdf = gdf[gdf.geometry.is_valid]  # Drop invalid geometries
    gdf["area_sqm"] = gdf.to_crs(epsg=3857).geometry.area
    gdf = gdf[gdf["area_sqm"] > 10]  # Remove slivers
    gdf["updated_at"] = pd.Timestamp.now()

    # Load
    gdf.to_parquet(output_path, compression="zstd", write_covering_bbox=True)
    print(f"Exported {len(gdf)} parcels to {output_path}")

etl_parcels(
    "https://data.sfgov.org/api/geospatial/parcels?method=export&type=GeoJSON",
    "sf_parcels_clean.parquet",
    bbox=(-122.55, 37.65, -122.30, 37.85),
)
```

### 13.2 Dagster for Spatial Data Pipelines

Dagster is a modern data orchestrator well-suited for spatial ETL. Its asset-based model maps naturally to geospatial workflows where datasets depend on each other.

```python
# dagster_pipeline.py
from dagster import asset, Definitions, define_asset_job, ScheduleDefinition
import geopandas as gpd
import duckdb

@asset(description="Raw building footprints from Overture Maps")
def raw_buildings():
    """Download building footprints for the study area."""
    conn = duckdb.connect()
    conn.execute("INSTALL spatial; LOAD spatial; INSTALL httpfs; LOAD httpfs;")
    conn.execute("SET s3_region = 'us-west-2';")

    conn.execute("""
        COPY (
            SELECT *
            FROM read_parquet(
                's3://overturemaps-us-west-2/release/2024-12-18.0/theme=buildings/type=building/*'
            )
            WHERE bbox.xmin >= -122.55 AND bbox.xmax <= -122.25
              AND bbox.ymin >= 37.65 AND bbox.ymax <= 37.95
        ) TO '/data/raw/buildings.parquet' (FORMAT PARQUET, COMPRESSION ZSTD);
    """)
    return "/data/raw/buildings.parquet"

@asset(deps=[raw_buildings], description="Cleaned and enriched buildings")
def clean_buildings():
    """Clean geometries, compute areas, classify building types."""
    gdf = gpd.read_parquet("/data/raw/buildings.parquet")

    # Clean
    gdf = gdf[gdf.geometry.is_valid]
    gdf = gdf[~gdf.geometry.is_empty]

    # Enrich
    gdf["area_sqm"] = gdf.to_crs(epsg=3857).geometry.area
    gdf["size_class"] = pd.cut(gdf["area_sqm"],
                                bins=[0, 100, 500, 2000, float("inf")],
                                labels=["small", "medium", "large", "xlarge"])

    gdf.to_parquet("/data/clean/buildings.parquet",
                   compression="zstd", write_covering_bbox=True)
    return "/data/clean/buildings.parquet"

@asset(deps=[clean_buildings], description="H3 hexbin aggregation of buildings")
def building_hexbins():
    """Aggregate buildings to H3 resolution 8 hexagons."""
    conn = duckdb.connect()
    conn.execute("INSTALL spatial; LOAD spatial; INSTALL h3 FROM community; LOAD h3;")

    conn.execute("""
        COPY (
            SELECT
                h3_latlng_to_cell(
                    ST_Y(ST_Centroid(ST_GeomFromWKB(geometry))),
                    ST_X(ST_Centroid(ST_GeomFromWKB(geometry))),
                    8
                ) AS h3_cell,
                COUNT(*) AS building_count,
                AVG(area_sqm) AS avg_area,
                SUM(area_sqm) AS total_area
            FROM read_parquet('/data/clean/buildings.parquet')
            GROUP BY h3_cell
        ) TO '/data/analytics/building_hexbins.parquet' (FORMAT PARQUET);
    """)
    return "/data/analytics/building_hexbins.parquet"

# Schedule daily refresh
daily_refresh = define_asset_job("daily_refresh", selection="*")
daily_schedule = ScheduleDefinition(job=daily_refresh, cron_schedule="0 6 * * *")

defs = Definitions(
    assets=[raw_buildings, clean_buildings, building_hexbins],
    schedules=[daily_schedule],
)
```

### 13.3 Airflow for Spatial Pipelines

```python
# airflow_dag.py
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from datetime import datetime

def download_sentinel():
    import pystac_client
    catalog = pystac_client.Client.open("https://earth-search.aws.element84.com/v1")
    items = catalog.search(
        collections=["sentinel-2-l2a"],
        bbox=[-122.5, 37.5, -122.0, 38.0],
        datetime="{{ ds }}/{{ next_ds }}",
        query={"eo:cloud_cover": {"lt": 20}},
    ).item_collection()
    items.save_object("/data/stac/items_{{ ds }}.json")

def compute_ndvi():
    import odc.stac, pystac
    items = pystac.ItemCollection.from_file("/data/stac/items_{{ ds }}.json")
    ds = odc.stac.load(items, bands=["red", "nir"], resolution=10)
    ndvi = (ds.nir - ds.red) / (ds.nir + ds.red)
    ndvi.median(dim="time").rio.to_raster("/data/ndvi/ndvi_{{ ds }}.tif", driver="COG")

with DAG("sentinel_ndvi", start_date=datetime(2024, 1, 1), schedule="@weekly") as dag:
    download = PythonOperator(task_id="download", python_callable=download_sentinel)
    ndvi = PythonOperator(task_id="compute_ndvi", python_callable=compute_ndvi)
    upload = BashOperator(task_id="upload",
                          bash_command="aws s3 cp /data/ndvi/ndvi_{{ ds }}.tif s3://bucket/ndvi/")
    download >> ndvi >> upload
```

### 13.4 Kart: Version Control for Geospatial Data

Kart (by Koordinates) is Git for geospatial data. It tracks changes to vector datasets (GeoPackage, PostGIS) with full diff, merge, and branch capabilities.

```bash
# Install
brew install kart  # macOS
# or: pip install kart

# Initialize a repo from a GeoPackage
kart init my-parcels
cd my-parcels
kart import parcels.gpkg

# Check status
kart status
# On branch main
# Changes not yet committed:
#   parcels: 45,230 features

# Commit
kart commit -m "Initial import of parcels dataset"

# Create a branch for updates
kart checkout -b quarterly-update

# Import updated data
kart import --replace-existing updated_parcels.gpkg

# Diff: see what changed
kart diff main...quarterly-update --output-format=json
# Shows: 234 inserts, 89 updates, 12 deletes

# Merge
kart checkout main
kart merge quarterly-update

# Export back to GeoPackage
kart export gpkg output.gpkg

# Push to remote (like Git)
kart remote add origin https://kart.koordinates.com/my-org/parcels
kart push origin main
```

### 13.5 CI/CD for Spatial Data

```yaml
# .github/workflows/spatial-data-ci.yml
name: Spatial Data CI

on:
  push:
    paths:
      - 'data/**'
      - 'etl/**'

jobs:
  validate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: pip install geopandas pyarrow duckdb shapely

      - name: Validate GeoParquet files
        run: |
          python -c "
          import geopandas as gpd
          import pyarrow.parquet as pq
          from pathlib import Path

          for f in Path('data').glob('**/*.parquet'):
              print(f'Validating {f}...')
              # Check readable
              gdf = gpd.read_parquet(f)
              assert len(gdf) > 0, f'{f} is empty'

              # Check valid geometries
              invalid = (~gdf.geometry.is_valid).sum()
              assert invalid == 0, f'{f} has {invalid} invalid geometries'

              # Check CRS
              assert gdf.crs is not None, f'{f} has no CRS'

              # Check GeoParquet metadata
              pf = pq.ParquetFile(f)
              meta = pf.schema_arrow.pandas_metadata
              print(f'  OK: {len(gdf)} features, CRS={gdf.crs}')
          "

      - name: Run spatial queries
        run: |
          python -c "
          import duckdb
          conn = duckdb.connect()
          conn.execute('INSTALL spatial; LOAD spatial;')
          # Smoke test: verify data is queryable
          result = conn.execute('''
              SELECT COUNT(*) FROM read_parquet('data/buildings.parquet')
          ''').fetchone()
          print(f'Buildings count: {result[0]}')
          assert result[0] > 0
          "

  deploy:
    needs: validate
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
      - uses: actions/checkout@v4

      - name: Upload to S3
        run: |
          aws s3 sync data/ s3://my-spatial-data/latest/ \
            --exclude "*.shp" --exclude "*.dbf" --exclude "*.shx" \
            --include "*.parquet" --include "*.fgb" --include "*.pmtiles"
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
```

### 13.6 Pipeline Architecture Patterns

```
Pattern 1: Simple ETL (< 10 GB)
================================
  Source (Shapefile/API)
       |
  GDAL/ogr2ogr --> GeoParquet
       |
  DuckDB Spatial (query/transform)
       |
  GeoParquet on S3

Pattern 2: Satellite Pipeline (STAC)
=====================================
  STAC Catalog Search
       |
  odc-stac / stackstac --> xarray (Dask-backed)
       |
  Compute (NDVI, classification, etc.)
       |
  COG on S3 / Zarr on S3

Pattern 3: Large-Scale Vector (> 50 GB)
========================================
  Source GeoParquet (partitioned)
       |
  Apache Sedona (Spark cluster)
       |
  Spatial Joins / Aggregations
       |
  Partitioned GeoParquet on S3

Pattern 4: Real-Time Ingest
============================
  Streaming Source (Kafka / MQTT)
       |
  Apache Flink / Kafka Streams
       |
  GeoMesa (HBase/Accumulo)
       |
  Query via GeoServer WFS or direct
```

---

## 14. Performance Benchmarks

### 14.1 Vector Format Read/Write Benchmarks

Test: 10 million polygons, 15 attributes, M2 MacBook Pro 32 GB RAM, SSD. Python 3.12.

| Format | Write (s) | Read Full (s) | Read Bbox 1% (s) | Read 3 Cols (s) | File Size (GB) |
|--------|-----------|---------------|-------------------|-----------------|----------------|
| Shapefile | 68 | 45 | 45 (full scan) | 45 (full scan) | 4.2 |
| GeoJSON | 142 | 110 | 110 (full scan) | 110 (full scan) | 8.7 |
| GeoJSONSeq | 125 | 95 | 95 (full scan) | 95 (full scan) | 8.5 |
| GeoPackage | 55 | 38 | 4.2 (r-tree) | 36 (no col prune) | 3.1 |
| FlatGeobuf | 35 | 25 | 0.6 (hilbert) | 25 (no col prune) | 2.8 |
| GeoParquet (Snappy) | 22 | 12 | 0.8 (covering) | 3.5 | 1.4 |
| GeoParquet (Zstd) | 28 | 13 | 0.9 (covering) | 3.7 | 1.1 |

### 14.2 Raster Format Benchmarks

Test: 10,000 x 10,000 pixels, 4 bands, Float32, M2 MacBook Pro.

| Format | Write (s) | Read Full (s) | Read 512x512 Window (s) | Remote Window (s) | File Size (MB) |
|--------|-----------|---------------|-------------------------|-------------------|----------------|
| GeoTIFF (strips) | 2.1 | 1.8 | 1.8 (full scan) | N/A | 1525 |
| GeoTIFF (tiled, deflate) | 3.2 | 2.3 | 0.02 | 0.15 | 980 |
| COG (deflate) | 3.5 | 2.3 | 0.02 | 0.12 | 1020 |
| COG (zstd) | 3.0 | 2.0 | 0.02 | 0.11 | 940 |
| Zarr (local, zstd) | 2.8 | 1.9 | 0.01 | N/A | 950 |
| Zarr (S3, zstd) | 4.5 | 8.0 | 0.25 | 0.25 | 950 |
| NetCDF4 (chunked) | 3.5 | 2.5 | 0.8 | N/A | 1100 |

### 14.3 Spatial Join Benchmarks

Test: Join 10M points to 50K polygons.

| Tool | Time (s) | RAM (GB) | Notes |
|------|----------|----------|-------|
| PostGIS (indexed) | 180 | 8 | GIST index on both tables |
| GeoPandas (sjoin) | 45 | 12 | In-memory, R-tree |
| DuckDB Spatial | 28 | 6 | Vectorized, predicate pushdown |
| Dask-GeoPandas (8 workers) | 18 | 4/worker | Hilbert partitioned |
| Apache Sedona (8 executors) | 12 | 4/executor | KDB-tree partition |

### 14.4 Query Engine Comparison

Test: Filter 500M building footprints by bbox, count by class.

| Engine | Query Time | Data Source | Notes |
|--------|-----------|-------------|-------|
| DuckDB (local Parquet) | 3.2s | GeoParquet on SSD | Single machine, covering pushdown |
| DuckDB (remote Parquet) | 8.5s | GeoParquet on S3 | HTTP range requests |
| PostGIS | 2.8s | PostGIS with GIST | Requires data import (hours) |
| Apache Sedona (4 nodes) | 4.1s | GeoParquet on S3 | Includes Spark overhead |
| BigQuery GIS | 5.2s | BigQuery native | Serverless, cold start |
| Snowflake Geospatial | 6.8s | Internal tables | Warehouse spinning cost |

### 14.5 Memory Efficiency Comparison

Test: Process 20 GB GeoParquet file, count features per county.

| Tool | Peak RAM | Time | Approach |
|------|----------|------|----------|
| GeoPandas | 48 GB (OOM on 32 GB) | N/A | Loads everything |
| DuckDB | 4 GB | 25s | Streaming aggregation |
| Dask-GeoPandas (8 partitions) | 6 GB | 38s | Partition-at-a-time |
| Apache Sedona (4 nodes, 8 GB each) | 8 GB/node | 15s | Distributed |
| Polars + GeoPolars | 8 GB | 22s | Streaming, columnar |

---

## 15. Cost Optimization

### 15.1 Free Tier Comparison

| Platform | Free Compute | Free Storage | Data Catalog | Best For |
|----------|-------------|-------------|--------------|----------|
| Google Earth Engine | Unlimited (noncommercial) | 250 GB assets | Petabyte catalog | Satellite analysis |
| Microsoft Planetary Computer | Free JupyterHub + Dask | Hub storage | Petabyte catalog | Multi-source analysis |
| AWS (Free Tier) | 750 hrs t2.micro/month (12 mo) | 5 GB S3 | Registry of Open Data | Custom infrastructure |
| Google Colab | Free GPU/TPU (limited) | 15 GB Drive | None built-in | Prototyping |
| Hugging Face Spaces | Free CPU (limited) | 50 GB (dataset hub) | Geospatial datasets | ML model hosting |
| Source Cooperative | N/A | Free for open data | Community datasets | Data publishing |
| DuckDB (local) | Your machine | Your disk | N/A | Local analytics |

### 15.2 Serverless Patterns

Serverless architectures minimize cost by charging only for actual compute time.

```python
# Pattern 1: AWS Lambda + DuckDB for on-demand spatial queries
# lambda_function.py
import duckdb
import json

def handler(event, context):
    bbox = event["queryStringParameters"]
    conn = duckdb.connect()
    conn.execute("INSTALL spatial; LOAD spatial; INSTALL httpfs; LOAD httpfs;")
    conn.execute("SET s3_region = 'us-west-2';")

    result = conn.execute(f"""
        SELECT id, names.primary AS name,
               ST_AsGeoJSON(ST_GeomFromWKB(geometry)) AS geojson
        FROM read_parquet('s3://my-data/buildings.parquet')
        WHERE bbox.xmin >= {bbox['xmin']}
          AND bbox.xmax <= {bbox['xmax']}
          AND bbox.ymin >= {bbox['ymin']}
          AND bbox.ymax <= {bbox['ymax']}
        LIMIT 1000
    """).fetchdf()

    features = []
    for _, row in result.iterrows():
        features.append({
            "type": "Feature",
            "properties": {"id": row["id"], "name": row["name"]},
            "geometry": json.loads(row["geojson"]),
        })

    return {
        "statusCode": 200,
        "headers": {"Content-Type": "application/geo+json"},
        "body": json.dumps({"type": "FeatureCollection", "features": features}),
    }
```

```yaml
# serverless.yml (Serverless Framework)
service: spatial-query
provider:
  name: aws
  runtime: python3.12
  memorySize: 1024
  timeout: 30
  iamRoleStatements:
    - Effect: Allow
      Action: s3:GetObject
      Resource: arn:aws:s3:::my-data/*

functions:
  query:
    handler: lambda_function.handler
    events:
      - httpApi:
          path: /query
          method: get
    layers:
      - arn:aws:lambda:us-west-2:xxx:layer:duckdb-python:1
```

```python
# Pattern 2: Google Cloud Functions + Earth Engine
# main.py
import ee
import functions_framework

@functions_framework.http
def compute_ndvi(request):
    ee.Initialize()
    params = request.get_json()

    point = ee.Geometry.Point(params["lon"], params["lat"])
    image = (
        ee.ImageCollection("COPERNICUS/S2_SR_HARMONIZED")
        .filterDate(params["start"], params["end"])
        .filterBounds(point)
        .filter(ee.Filter.lt("CLOUDY_PIXEL_PERCENTAGE", 20))
        .median()
    )

    ndvi = image.normalizedDifference(["B8", "B4"])
    value = ndvi.reduceRegion(
        reducer=ee.Reducer.mean(),
        geometry=point.buffer(params.get("radius", 500)),
        scale=10,
    ).getInfo()

    return {"ndvi": value.get("nd", None)}
```

### 15.3 Caching Strategies

| Strategy | When to Use | Implementation | Estimated Savings |
|----------|-------------|---------------|-------------------|
| Local Parquet cache | Repeated queries on same data | `gpd.read_parquet()` caches to local disk | 10-50x for remote data |
| DuckDB persistent DB | Repeated complex queries | `duckdb.connect('cache.db')` | 5-20x for complex joins |
| S3 requester-pays awareness | Accessing public data | Set `requester_pays=True` in storage_options | Avoid surprise bills |
| Tile cache (PMTiles) | Serving maps | Pre-compute + static hosting on S3 | 100x vs dynamic tile server |
| STAC item cache | Repeated catalog searches | Cache `ItemCollection` to JSON | Avoid API rate limits |
| CDN for COGs | Global access patterns | CloudFront/Fastly in front of S3 | Lower egress, faster access |
| Zarr consolidated metadata | Opening large Zarr stores | `consolidated=True` on write | 1 request vs 1000s |

```python
# Example: Local caching layer for remote data
from pathlib import Path
import hashlib
import geopandas as gpd

CACHE_DIR = Path("/tmp/geo_cache")
CACHE_DIR.mkdir(exist_ok=True)

def cached_read_parquet(url: str, bbox=None, **kwargs) -> gpd.GeoDataFrame:
    """Read GeoParquet with local caching."""
    cache_key = hashlib.md5(f"{url}_{bbox}".encode()).hexdigest()
    cache_path = CACHE_DIR / f"{cache_key}.parquet"

    if cache_path.exists():
        print(f"Cache hit: {cache_path}")
        return gpd.read_parquet(cache_path)

    print(f"Cache miss, downloading: {url}")
    gdf = gpd.read_parquet(url, bbox=bbox, **kwargs)
    gdf.to_parquet(cache_path, compression="zstd")
    return gdf
```

### 15.4 Choosing the Right Platform

```
                        Complexity of Pipeline
                    Low  High
                                            
              Free    DuckDB Local      Dagster + DuckDB
                 GEE (free tier)   MPC Hub + Dask
  Cost                Colab + GeoPandas Airflow + GDAL
                                  
              Paid    BigQuery GIS      Sedona on Databricks
                      Snowflake Geo     Custom Spark on EMR
                                        GEE Commercial
```

Decision flowchart:

1. **Do you need satellite data analysis?**
   - Yes, simple analysis: **Google Earth Engine** (free)
   - Yes, complex multi-source: **Planetary Computer** (free) or **custom STAC pipeline**

2. **Is your vector data < 10 GB?**
   - Yes: **DuckDB** (local, free, fast)

3. **Is your vector data 10-100 GB?**
   - Yes, single machine: **DuckDB** or **Dask-GeoPandas**
   - Yes, cloud: **DuckDB on EC2** or **Planetary Computer Hub**

4. **Is your vector data > 100 GB?**
   - Yes: **Apache Sedona** on Databricks/EMR/Dataproc

5. **Do you need a managed SQL interface?**
   - BigQuery GIS or Snowflake Geospatial (pay-per-query)

6. **Do you need real-time streaming?**
   - Apache Flink + GeoMesa or Apache Sedona Streaming

### 15.5 Cost Estimation Examples

| Scenario | Platform | Monthly Cost |
|----------|----------|-------------|
| Analyze Sentinel-2 NDVI for 1 country, weekly | GEE (free tier) | $0 |
| Query 500M buildings on demand, 100 queries/day | DuckDB on t3.xlarge + S3 | ~$120 |
| Daily ETL: ingest 5 GB parcels, transform, serve | Lambda + S3 + CloudFront | ~$30 |
| Planetary-scale land cover classification | Sedona on EMR (10 nodes, 4 hrs/week) | ~$500 |
| Tile serving: 10M tiles, 1M requests/month | PMTiles on S3 + CloudFront | ~$15 |
| STAC search + COG access (research, 50 GB/month) | Planetary Computer Hub | $0 |

---

## Cross-Reference Index

This section maps topics in this guide to related content elsewhere in the repository.

| Topic | Related Pages |
|-------|--------------|
| Python libraries (GeoPandas, Rasterio, Shapely) | [data-analysis/python-stack.md](python-stack.md) |
| R spatial stack (sf, terra) | [data-analysis/r-stack.md](r-stack.md) |
| Spatial statistics and autocorrelation | [data-analysis/spatial-statistics.md](spatial-statistics.md) |
| ML for geospatial (classification, deep learning) | [data-analysis/ml-gis.md](ml-gis.md) |
| Workflow templates (suitability, change detection) | [data-analysis/workflow-templates.md](workflow-templates.md) |
| Satellite imagery sources | [data-sources/satellite-imagery.md](../data-sources/satellite-imagery.md) |
| Vector data sources (OSM, Natural Earth) | [data-sources/vector-data.md](../data-sources/vector-data.md) |
| Elevation and terrain data | [data-sources/elevation-terrain.md](../data-sources/elevation-terrain.md) |
| Climate and weather data | [data-sources/climate-weather.md](../data-sources/climate-weather.md) |
| Cloud platforms (AWS, GCP, Azure) | [tools/cloud-platforms.md](../tools/cloud-platforms.md) |
| Spatial databases (PostGIS, SpatiaLite) | [tools/spatial-databases.md](../tools/spatial-databases.md) |
| ETL and data engineering tools | [tools/etl-data-engineering.md](../tools/etl-data-engineering.md) |
| CLI tools (GDAL, OGR, rio) | [tools/cli-tools.md](../tools/cli-tools.md) |
| Python libraries catalog | [tools/python-libraries.md](../tools/python-libraries.md) |
| AI/ML geospatial tools | [tools/ai-ml-geospatial.md](../tools/ai-ml-geospatial.md) |
| Browser data format loading | [js-bindbox/data-formats-loading.md](../js-bindbox/data-formats-loading.md) |
| Tile servers (PMTiles, vector tiles) | [js-bindbox/tile-servers.md](../js-bindbox/tile-servers.md) |
| Browser spatial analysis | [js-bindbox/spatial-analysis.md](../js-bindbox/spatial-analysis.md) |
| Performance optimization (frontend) | [js-bindbox/performance-optimization.md](../js-bindbox/performance-optimization.md) |

---

## Glossary

| Term | Definition |
|------|-----------|
| **COG** | Cloud-Optimized GeoTIFF. A GeoTIFF with internal tiling, overviews, and header-first layout enabling HTTP range requests. |
| **Covering** | GeoParquet 1.1 feature: per-row bounding box columns enabling spatial predicate pushdown at the Parquet row-group level. |
| **GeoArrow** | Apache Arrow extension types for geometry, enabling zero-copy exchange between tools without WKB serialization. |
| **GeoParquet** | Apache Parquet with standardized geospatial metadata (geometry column encoding, CRS, bbox). |
| **H3** | Uber's Hexagonal Hierarchical Spatial Index. Partitions the globe into hexagons at 16 resolution levels. |
| **Hilbert Curve** | A space-filling curve that maps 2D space to 1D while preserving locality. Used for spatial indexing in FlatGeobuf and spatial partitioning in Dask-GeoPandas. |
| **Predicate Pushdown** | Query optimization that pushes filter conditions into the storage layer, so only matching data is read. |
| **Row Group** | A horizontal partition of a Parquet file. Each row group stores column chunks with min/max statistics enabling skip-scan. |
| **STAC** | SpatioTemporal Asset Catalog. A JSON specification for describing geospatial data so it is searchable and interoperable. |
| **VRT** | GDAL Virtual Raster. An XML file describing a mosaic of rasters without copying data. |
| **WKB** | Well-Known Binary. A compact binary encoding for geometry used in GeoParquet, PostGIS, and other systems. |
| **Zarr** | A format for chunked, compressed, N-dimensional arrays. The cloud-native alternative to NetCDF/HDF5. |

---

## Version History

| Date | Changes |
|------|---------|
| 2026-02-25 | Initial release covering GeoParquet 1.1, DuckDB 1.2, STAC, xarray, Sedona, Overture Maps 2024-12, and full pipeline patterns. |

---

[Back to Data Analysis Index](README.md) | [Back to Main README](../README.md)
